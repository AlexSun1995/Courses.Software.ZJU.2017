{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175881\n"
     ]
    }
   ],
   "source": [
    "# encoding=utf-8\n",
    "# 浪潮之巅　实体分析\n",
    "\n",
    "import jieba\n",
    "import codecs\n",
    "with codecs.open('clzd.txt','r','utf-8') as f:\n",
    "    all_words = f.readlines()\n",
    "    lines = []\n",
    "    cnt = 0\n",
    "    for line in all_words:\n",
    "        if line != \"\\r\\n\":\n",
    "            line = line[0:-2]\n",
    "            lines.append(line)\n",
    "            cnt += len(line)\n",
    "    #print(lines)\n",
    "    print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 1.690 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9972\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "word_cnt = collections.defaultdict(int)\n",
    "words_list = []\n",
    "for line in lines:\n",
    "    li = jieba.cut(line)\n",
    "    for item in li:\n",
    "        word_cnt[item] += 1\n",
    "#print(word_cnt)\n",
    "print(len(word_cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9659\n",
      "[('人', 294), ('年', 269), ('美国', 249), ('硅谷', 237), ('中', 234), ('后', 215), ('英特尔', 211), ('技术', 210), ('很多', 209), ('投资', 208), ('摩托罗拉', 203), ('已经', 194), ('发展', 193), ('计算机', 192), ('做', 183), ('会', 182), ('世界', 182), ('惠普', 179), ('大', 175), ('处理器', 174), ('.', 170), ('可能', 163), ('时', 161), ('思科', 156), ('太阳', 155), ('AT&T', 155), ('互联网', 154), ('还', 149), ('今天', 149), ('苹果', 148), ('新', 147), ('产品', 147), ('不是', 142), ('成功', 142), ('手机', 138), ('领域', 137), ('现在', 137), ('%', 136), ('微机', 136), ('最', 135)]\n"
     ]
    }
   ],
   "source": [
    "stopwords = set()\n",
    "# 载入停用词(哈工大停用词)\n",
    "with codecs.open('hit_stopwords.txt','r','utf-8') as f:\n",
    "    words = f.readlines()\n",
    "    for word in words:\n",
    "        stopwords.add(word[:-2])\n",
    "        \n",
    "word_list = [(k,v) for k,v in word_cnt.items() if k not in stopwords]\n",
    "print(len(word_list))\n",
    "\n",
    "sorted_list = sorted(word_list, key=lambda t:(-t[1]))\n",
    "print(sorted_list[10:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "LTP_DIR_PATH = '/home/alexsun/ML/nlp/ltp_model/ltp_data'\n",
    "pos_model_path = os.path.join(LTP_DIR_PATH, 'pos.model')\n",
    "# 词性标注模型路径，模型名称为`pos.model`\n",
    "\n",
    "from pyltp import Postagger\n",
    "postagger = Postagger() # 初始化实例\n",
    "postagger.load(pos_model_path)  # 加载模型\n",
    "post_lines = []\n",
    "# 分词后的列表，每项代表一段分词结果\n",
    "paragh_words = []\n",
    "for line in lines:\n",
    "    # 不考虑太短的句子\n",
    "    if len(line) < 20:\n",
    "        continue\n",
    "    li = list(jieba.cut(line))\n",
    "    new_li = []\n",
    "    for item in li:\n",
    "        if item not in stopwords:\n",
    "            new_li.append(item)\n",
    "    paragh_words.append(new_li)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['v', 'v', 'v', 'v', 'nt', 'v', 'nz', 'n', 'n', 'i', 'z', 'nt', 'd', 'v', 'i', 'ws', 'a', 'v', 'v', 'm', 'd', 'v', 'n', 'i', 'd', 'nt', 'n', 'v', 'nt', 'ws', 'nt', 'a', 'nh', 'n', 'nl', 'nh', 'n', 'm', 'n', 'd', 'v', 'v', 'a', 'n', 'n', 'n', 'n', 'n', 'n', 'ns', 'd', 'v', 'ws', 'n', 'v', 'n', 'v', 'd', 'nt', 'r', 'n', 'ws', 'd', 'm', 'a', 'v', 'n', 'm', 'q', 'n', 'v', 'nt', 'nd', 'v', 'd', 'v', 'v', 'ni', 'd', 'v', 'a', 'n', 'd', 'n', 'v', 'nd', 'v', 'n']\n",
      "[' ', ' ', ' ', ' ', '一九九五年', '说', 'AT&T', '公司', '顶峰', '接下来', '短短的', '十年', '便', '分崩离析', '不复存在', 'AT&T', '不紧不慢', '向上', '走过', '百年', '才', '爬', '顶点', '走下坡路', '却', '十年', '时间', '注', '今天', 'AT&T', '当年', '小', '贝尔', '公司', '西南', '贝尔', '公司', '几次', '小吃', '大', '合并', '出', '类似', '水电', '公司', '设施', '服务公司', '类', '公司', '美国', '统统', '称为', 'utility', '公司', '毫无', '技术', '可言', '其实', '一九九五年', '这十来', '年间', 'AT&T', '本来', '两次', '绝佳', '发展', '机遇', '2000', '年', '网络', '革命', '九十年代', '中期', '延续', '至今', '无线通信', '飞跃', 'AT&T', '没有', '利用', '好', '机会', '反而', '两场', '变革', '中', '丢', '性命']\n",
      "713\n"
     ]
    }
   ],
   "source": [
    "# 每项为一段文字的词性标签\n",
    "all_paragh_tags = []\n",
    "# 每项为一段文字的分词结果\n",
    "paragh_words_list = paragh_words\n",
    "for line in paragh_words_list:\n",
    "    line_tages = postagger.postag(line)\n",
    "    all_paragh_tags.append(list(line_tages))\n",
    "print(all_paragh_tags[13])\n",
    "print(paragh_words_list[13])\n",
    "print(len(all_paragh_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "713\n"
     ]
    }
   ],
   "source": [
    "#　命名实体识别\n",
    "\n",
    "# 命名实体识别模型路径，模型名称为`pos.model`\n",
    "ner_model_path = os.path.join(LTP_DIR_PATH, 'ner.model')\n",
    "from pyltp import NamedEntityRecognizer\n",
    "recognizer = NamedEntityRecognizer() #模型初始化\n",
    "recognizer.load(ner_model_path)\n",
    "len_paragh = len(all_paragh_tags)\n",
    "# 每一项为一段文字的实体标注结果\n",
    "netags_all = []\n",
    "for i in range(len_paragh):\n",
    "    netags = recognizer.recognize(paragh_words_list[i],all_paragh_tags[i])\n",
    "    netags_all.append(list(netags))\n",
    "recognizer.release()\n",
    "print(len(netags_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['O', 'O', 'O', 'S-Nh', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'S-Nh', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'S-Nh', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'B-Ni', 'I-Ni', 'I-Ni', 'E-Ni', 'O', 'O', 'B-Ni', 'I-Ni', 'E-Ni', 'B-Ni', 'I-Ni', 'I-Ni', 'I-Ni', 'I-Ni', 'I-Ni', 'I-Ni', 'I-Ni', 'E-Ni', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'S-Nh', 'O']]\n"
     ]
    }
   ],
   "source": [
    "print(netags_all[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 构建段落实体集合(对每个段落，只保留实体词)\n",
    "# 每一项是\n",
    "paragh_entity_words = []\n",
    "for i in range(len_paragh):\n",
    "    this_paragh_words = paragh_words_list[i]\n",
    "    this_paragh_ntags = netags_all[i]\n",
    "    li = []\n",
    "    for i in range(len(this_paragh_ntags)):\n",
    "        item = this_paragh_ntags[i]\n",
    "        if item != 'O':\n",
    "            li.append(this_paragh_words[i])\n",
    "    paragh_entity_words.append(li)\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "paragh_entity_word_set = []\n",
    "for i in range(len(paragh_entity_words)):\n",
    "    paragh_entity_word_set.append(set(paragh_entity_words[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "write() argument must be str, not bytes",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-d77b88e1544b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'entity_set.pickle'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparagh_entity_word_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"success!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: write() argument must be str, not bytes"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('entity_set.pickle','w') as f:\n",
    "    pickle.dump(paragh_entity_word_set,f)\n",
    "    print(\"success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
