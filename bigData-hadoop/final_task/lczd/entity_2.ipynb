{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175881\n"
     ]
    }
   ],
   "source": [
    "# encoding=utf-8\n",
    "# 浪潮之巅　实体分析\n",
    "\n",
    "import jieba\n",
    "import codecs\n",
    "with codecs.open('clzd.txt','r','utf-8') as f:\n",
    "    all_words = f.readlines()\n",
    "    lines = []\n",
    "    cnt = 0\n",
    "    for line in all_words:\n",
    "        if line != \"\\r\\n\":\n",
    "            line = line[0:-2]\n",
    "            lines.append(line)\n",
    "            cnt += len(line)\n",
    "    #print(lines)\n",
    "    print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 1.665 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9972\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "word_cnt = collections.defaultdict(int)\n",
    "words_list = []\n",
    "for line in lines:\n",
    "    li = jieba.cut(line)\n",
    "    for item in li:\n",
    "        word_cnt[item] += 1\n",
    "#print(word_cnt)\n",
    "print(len(word_cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9659\n",
      "[('人', 294), ('年', 269), ('美国', 249), ('硅谷', 237), ('中', 234), ('后', 215), ('英特尔', 211), ('技术', 210), ('很多', 209), ('投资', 208), ('摩托罗拉', 203), ('已经', 194), ('发展', 193), ('计算机', 192), ('做', 183), ('会', 182), ('世界', 182), ('惠普', 179), ('大', 175), ('处理器', 174), ('.', 170), ('可能', 163), ('时', 161), ('思科', 156), ('太阳', 155), ('AT&T', 155), ('互联网', 154), ('今天', 149), ('还', 149), ('苹果', 148), ('产品', 147), ('新', 147), ('不是', 142), ('成功', 142), ('手机', 138), ('现在', 137), ('领域', 137), ('%', 136), ('微机', 136), ('最', 135)]\n"
     ]
    }
   ],
   "source": [
    "stopwords = set()\n",
    "# 载入停用词(哈工大停用词)\n",
    "with codecs.open('hit_stopwords.txt','r','utf-8') as f:\n",
    "    words = f.readlines()\n",
    "    for word in words:\n",
    "        stopwords.add(word[:-2])\n",
    "        \n",
    "word_list = [(k,v) for k,v in word_cnt.items() if k not in stopwords]\n",
    "print(len(word_list))\n",
    "\n",
    "sorted_list = sorted(word_list, key=lambda t:(-t[1]))\n",
    "print(sorted_list[10:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "LTP_DIR_PATH = '/home/alexsun/ML/nlp/ltp_model/ltp_data'\n",
    "pos_model_path = os.path.join(LTP_DIR_PATH, 'pos.model')\n",
    "# 词性标注模型路径，模型名称为`pos.model`\n",
    "\n",
    "from pyltp import Postagger\n",
    "postagger = Postagger() # 初始化实例\n",
    "postagger.load(pos_model_path)  # 加载模型\n",
    "post_lines = []\n",
    "# 分词后的列表，每项代表一段分词结果\n",
    "paragh_words = []\n",
    "for line in lines:\n",
    "    # 不考虑太短的句子\n",
    "    if len(line) < 20:\n",
    "        continue\n",
    "    li = list(jieba.cut(line))\n",
    "    new_li = []\n",
    "    for item in li:\n",
    "        if item not in stopwords:\n",
    "            new_li.append(item)\n",
    "    paragh_words.append(new_li)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['v', 'v', 'v', 'v', 'nt', 'v', 'nz', 'n', 'n', 'i', 'z', 'nt', 'd', 'v', 'i', 'ws', 'a', 'v', 'v', 'm', 'd', 'v', 'n', 'i', 'd', 'nt', 'n', 'v', 'nt', 'ws', 'nt', 'a', 'nh', 'n', 'nl', 'nh', 'n', 'm', 'n', 'd', 'v', 'v', 'a', 'n', 'n', 'n', 'n', 'n', 'n', 'ns', 'd', 'v', 'ws', 'n', 'v', 'n', 'v', 'd', 'nt', 'r', 'n', 'ws', 'd', 'm', 'a', 'v', 'n', 'm', 'q', 'n', 'v', 'nt', 'nd', 'v', 'd', 'v', 'v', 'ni', 'd', 'v', 'a', 'n', 'd', 'n', 'v', 'nd', 'v', 'n']\n",
      "[' ', ' ', ' ', ' ', '一九九五年', '说', 'AT&T', '公司', '顶峰', '接下来', '短短的', '十年', '便', '分崩离析', '不复存在', 'AT&T', '不紧不慢', '向上', '走过', '百年', '才', '爬', '顶点', '走下坡路', '却', '十年', '时间', '注', '今天', 'AT&T', '当年', '小', '贝尔', '公司', '西南', '贝尔', '公司', '几次', '小吃', '大', '合并', '出', '类似', '水电', '公司', '设施', '服务公司', '类', '公司', '美国', '统统', '称为', 'utility', '公司', '毫无', '技术', '可言', '其实', '一九九五年', '这十来', '年间', 'AT&T', '本来', '两次', '绝佳', '发展', '机遇', '2000', '年', '网络', '革命', '九十年代', '中期', '延续', '至今', '无线通信', '飞跃', 'AT&T', '没有', '利用', '好', '机会', '反而', '两场', '变革', '中', '丢', '性命']\n",
      "713\n"
     ]
    }
   ],
   "source": [
    "# 每项为一段文字的词性标签\n",
    "all_paragh_tags = []\n",
    "# 每项为一段文字的分词结果\n",
    "paragh_words_list = paragh_words\n",
    "for line in paragh_words_list:\n",
    "    line_tages = postagger.postag(line)\n",
    "    all_paragh_tags.append(list(line_tages))\n",
    "print(all_paragh_tags[13])\n",
    "print(paragh_words_list[13])\n",
    "print(len(all_paragh_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "713\n"
     ]
    }
   ],
   "source": [
    "#　命名实体识别\n",
    "\n",
    "# 命名实体识别模型路径，模型名称为`pos.model`\n",
    "ner_model_path = os.path.join(LTP_DIR_PATH, 'ner.model')\n",
    "from pyltp import NamedEntityRecognizer\n",
    "recognizer = NamedEntityRecognizer() #模型初始化\n",
    "recognizer.load(ner_model_path)\n",
    "len_paragh = len(all_paragh_tags)\n",
    "# 每一项为一段文字的实体标注结果\n",
    "netags_all = []\n",
    "for i in range(len_paragh):\n",
    "    netags = recognizer.recognize(paragh_words_list[i],all_paragh_tags[i])\n",
    "    netags_all.append(list(netags))\n",
    "recognizer.release()\n",
    "print(len(netags_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'S-Ns', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'S-Nh', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'S-Nh', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'S-Ni', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Ni', 'E-Ni', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'S-Ni', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'B-Ni', 'E-Ni', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'S-Nh', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'S-Nh', 'O', 'S-Nh', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'S-Nh', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Ns', 'E-Ns', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'B-Ni', 'E-Ni', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'S-Ns', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Nh', 'E-Nh', 'O', 'O', 'O', 'O', 'S-Ns', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'S-Nh', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'S-Ni', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'S-Nh', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'S-Ns', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'S-Ns', 'O', 'O', 'O', 'S-Ns', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'S-Ns', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'S-Nh', 'O', 'O', 'S-Ns', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'S-Nh', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]\n"
     ]
    }
   ],
   "source": [
    "# print(netags_all[3])\n",
    "# print(paragh_words_list[3])\n",
    "# print(len(netags_all[3]))\n",
    "# print(len(paragh_words_list[3]))\n",
    "# print(netags_all[33:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['余辉'], ['吴军', '吴军'], [], [], ['高盛'], ['弗伦'], [], [], [], [], ['梅毅强', '江泽民'], [], ['余辉'], [], [], ['道琼斯'], ['小得多'], [], [], ['余辉'], [], [], ['弗伦'], [], [], [], ['阿尔卡特'], [], [], ['余辉'], [], [], ['汤普生'], [], [], ['巴菲特', '巴菲特'], [], [], ['卡斯帕罗夫', '卡斯帕罗夫', '卡斯帕罗夫', '卡斯帕罗夫'], [], [], ['托马斯'], [], ['罗斯福'], [], ['杜鲁门'], ['老华生'], [], [], [], ['奇莱'], ['小华生', '诺伊曼'], [], [], [], [], [], [], [], ['凯利', '凯利'], [], [], [], ['老华生', '小华生', '费劲儿'], [], ['盖茨', '拿破仑', '凯撒', '盖茨'], ['盖茨', '盖茨', '盖茨', '盖茨', '盖茨'], [], [], ['郭士纳', '郭士纳', '郭士纳'], ['郭士纳', '郭士纳'], [], ['郭士纳', '郭士纳', '郭士纳', '郭士纳'], ['郭士纳', '郭士纳'], ['郭士纳', '郭士纳'], ['郭士纳', '郭士纳', '郭士纳', '郭士纳', '郭士纳'], [], [], [], [], [], ['郭士纳', '巴菲特'], [], [], [], [], [], [], [], [], [], ['乔布斯'], ['阿甘'], ['蒂诺'], [], ['乔布斯', '乔布斯'], ['乔布斯', '乔布斯', '乔布斯', '乔布斯', '乔布斯'], ['乔布斯', '乔布斯', '乔布斯', '乔布斯', '乔布斯', '乔布斯'], ['乔布斯', '韦恩', '韦恩', '乔布斯'], ['乔布斯', '乔布斯', '乔布斯', '托什'], ['托什'], ['托什', '帕罗'], ['乔布斯', '乔布斯'], ['乔布斯'], ['乔布斯', '库利', '库利', '乔布斯', '库利', '乔布斯'], ['库利', '牛顿', '乔布斯', '乔布斯', '乔布斯', '乔布斯', '乔布斯', '卢卡斯', '乔布斯'], ['乔布斯', '库利', '牛顿', '阿甘', '库利'], ['盖茨', '盖茨'], ['乔布斯', '乔布斯'], ['乔布斯'], ['乔布斯', '乔布斯', '乔布斯', '乔布斯'], ['乔布斯', '乔布斯', '乔布斯'], ['乔布斯', '乔布斯', '乔布斯', '库利'], ['库利', '乔布斯', '乔布斯'], [], ['乔布斯', '乔布斯'], ['乔布斯'], ['乔布斯'], ['乔布斯'], ['乔布斯', '乔布斯'], [], ['乔布斯'], ['安德森'], ['乔布斯', '盖茨', '盖茨'], [], [], ['托什', '托什', '乔布斯'], [], ['摩尔'], [], ['盖茨', '盖茨', '盖茨'], [], [], ['摩尔', '摩尔'], [], ['摩尔'], [], [], ['安迪'], [], ['比尔'], ['盖茨', '盖茨'], [], [], [], [], [], [], ['摩尔'], [], ['巴菲特'], [], [], ['摩尔', '贝尔', '查菲'], [], [], [], [], [], ['而安迪', '格罗夫'], ['罗伯特', '诺伊斯'], [], [], [], [], [], [], ['迦太基', '阿波罗', '托什'], [], [], [], ['格罗夫', '加尔文', '加尔文', '加尔文'], [], [], [], [], [], ['特森'], ['特森', '特森'], [], ['特森'], [], [], [], [], [], [], ['柳宗元'], [], ['彼得', '格'], [], [], [], [], [], [], [], ['李星', '韦钰', '钱伯斯'], ['奥纳多', '萨克'], [], [], [], [], [], [], ['阿尔卡特'], [], [], [], [], [], [], [], [], [], ['钱伯斯', '萨克', '斯坦福', '斯坦福', '萨克'], ['托尔斯泰'], [], ['阿尔卡特'], [], [], [], [], [], [], [], [], ['彼得', '格'], [], ['钱伯斯'], ['张家', '斯皮尔伯格', '张家', '张家'], [], [], [], ['凯莉', '菲奥莉娜', '菲奥莉娜'], ['菲奥莉娜'], [], ['克特', '斯坦福'], [], ['斯坦福'], [], [], [], ['图中', '巴菲特'], ['巴菲特'], [], [], [], [], [], ['郭士纳', '凯丽', '菲奥莉娜', '菲奥莉娜', '菲奥莉娜', '休伊特', '克特', '安捷伦'], [], ['安捷伦'], [], [], ['菲奥莉娜', '安捷伦'], [], ['凯丽', '菲奥莉娜'], ['菲奥莉娜'], ['菲奥莉娜', '安捷伦', '巴菲特', '安捷伦'], ['安捷伦', '菲奥莉娜', '王小二'], ['郭士纳', '格罗夫', '赫德', '菲奥莉娜'], ['菲奥莉娜', '菲奥莉娜', '菲奥莉娜'], ['菲奥丽娜', '菲奥莉娜', '休伊特'], ['菲奥莉娜'], [], ['菲奥莉娜', '郭台铭'], [], [], ['郭台铭', '松下幸之助', '任正非'], ['菲奥莉娜', '菲奥莉娜', '菲奥莉娜'], [], ['赫德', '赫德', '赫德', '赫德'], ['赫德', '赫德', '赫德', '赫德'], ['赫德', '赫德', '赫德'], [], [], ['赫德'], [], [], [], [], ['盖茨', '伊万'], [], ['张图'], ['加尔文', '保罗', '加尔文'], [], ['加尔文', '松下幸之助'], [], [], [], [], [], ['奇特'], [], [], [], [], [], [], [], [], [], ['李开复', '小巧'], [], [], ['卡尔文', '卡尔文', '杰克', '卡尔文', '摩尔'], [], [], [], ['小得多'], ['戈尔'], [], [], [], [], [], [], [], [], ['摩尔', '摩尔'], ['韦尔奇', '加尔文'], [], [], [], [], ['加尔文', '赫德'], [], [], ['乔布斯'], [], [], [], ['卡尔文', '卡尔文', '盖茨', '韦尔奇'], [], [], [], ['伯克利'], [], [], ['乔布斯', '吉姆', '克拉克'], [], [], [], ['麦克', '科恩'], [], ['关门大吉'], [], ['柯岩'], [], [], [], [], [], [], [], [], ['多明戈'], [], [], [], ['斯坦福', '乔布斯', '多如牛毛'], [], ['约翰', '约翰'], [], ['乔布斯'], [], [], [], [], [], [], [], [], ['摩尔', '戈登'], [], [], [], ['伯克利', '黄仁勋'], [], [], [], [], [], [], ['伯克利'], [], [], ['摩尔'], [], ['默尔', '辉瑞'], ['高达'], ['斯坦福'], [], [], ['茨威格'], ['舒昌善'], [], [], ['埃里克', '施密特', '韦恩', '罗森'], [], ['斯科特', '马可尼', '马可尼'], [], [], [], ['盖茨', '比尔盖茨'], [], ['马可尼', '马可尼', '马可尼'], [], ['施密特', '施密特', '施密特', '施密特', '施瓦茨', '马可尼'], [], [], [], ['马可尼'], [], ['张图'], ['马可尼'], ['盖茨'], [], [], ['马可尼'], [], ['马可尼', '马可尼', '马可尼', '施密特', '为荣', '马可尼', '马可尼', '马可尼'], [], [], [], [], [], [], [], [], [], [], [], [], [], ['帕洛'], [], [], [], [], ['于微软'], ['于微软', '于微软', '于微软'], [], [], [], [], ['盖茨', '布什'], [], [], [], [], ['克拉克', '盖茨', '盖茨'], ['盖茨', '盖茨', '盖茨', '盖茨'], ['盖茨', '盖茨', '盖茨', '盖茨', '布林', '盖茨'], ['康奈尔'], ['盖茨'], ['盖茨'], ['盖茨', '马可尼', '戴尔', '盖茨', '代尔', '代尔'], [], ['佩奇'], ['拉里', '佩奇'], ['佩奇'], ['佩奇'], [], [], [], ['佩奇'], [], ['杰克', '史密斯'], [], [], [], [], ['霍夫'], [], [], [], [], [], [], [], [], [], [], [], [], ['黑格尔', '恩格斯'], [], ['巴菲特'], [], [], [], ['王'], [], [], [], [], [], [], ['约翰', '伯乐', '朱敏', '斯坦福大学'], [], [], [], [], [], ['约翰'], [], [], ['强尼', '山姆', '亚平'], ['强尼', '迪克', '迪克'], ['强尼', '迪克', '迪克', '迪克', '强尼'], ['查理曼', '亚平', '亚平'], [], [], [], ['迪克', '山姆', '迪克'], ['亚平'], [], [], [], [], [], ['比尔'], [], [], ['亚平'], [], [], [], [], [], [], [], [], [], [], [], [], [], [], ['埃里森'], [], [], [], [], [], [], [], [], ['斯坦福'], [], [], ['奥尼尔', '施瓦辛格'], ['乔布斯'], [], [], [], [], [], [], [], [], [], [], ['唐纳德', '凡伦汀'], ['凡伦汀', '麦克', '利兹'], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], ['戈尔', '戈尔', '戈尔'], [], [], ['多尔', '多尔'], [], [], [], [], [], [], [], [], ['乔布斯'], ['马首是瞻'], ['和斯'], [], [], [], [], [], [], [], [], [], [], [], [], ['亨利'], [], [], [], [], [], [], [], [], ['辛弃疾'], [], ['佩奇', '霍尔茨'], [], [], [], [], [], [], [], ['格鲁曼'], ['卡迪拉克'], [], [], ['卡斯帕罗夫', '伍兹'], [], [], [], ['李开复'], [], [], [], ['约翰逊'], ['托什', '乔布斯', '托什', '盖茨'], [], [], [], [], [], [], [], [], [], ['JimAllchin'], [], [], ['约翰霍普金斯', '霍普金斯', '帕洛'], ['斯坦福'], ['胡佛', '斯坦福'], ['简', '斯坦福', '斯坦福', '乔丹', '乔丹', '斯坦福'], ['斯坦福', '克里夫兰', '乔丹', '斯坦福'], ['斯坦福', '斯坦福', '斯坦福'], ['斯坦福'], ['斯坦福'], ['康奈尔', '沃尔夫', '沃尔夫'], ['斯坦福', '斯坦福'], ['斯坦福', '斯坦福'], [], [], [], ['斯坦福'], ['斯坦福', '斯坦福'], ['李开复'], [], ['麻省', '斯坦福'], [], ['斯坦福', '斯坦福', '萨克', '约翰', '斯坦福', '查菲', '查菲'], ['斯坦福', '斯坦福大学', '约翰', '斯坦福', '斯坦福', '斯坦福'], [], [], ['斯坦福', '斯坦福'], [], [], ['斯坦福'], [], ['斯坦福'], ['尼克'], [], [], [], [], [], [], [], []]\n"
     ]
    }
   ],
   "source": [
    "# 构建段落人名实体集合\n",
    "# 每一项是一个段落中出现的人名实体的集合\n",
    "paragh_entity_name_words = []\n",
    "for i in range(len_paragh):\n",
    "    this_paragh_words = paragh_words_list[i]\n",
    "    this_paragh_ntags = netags_all[i]\n",
    "    li = []\n",
    "    for i in range(len(this_paragh_ntags)):\n",
    "        item = this_paragh_ntags[i]\n",
    "        if item == 'S-Nh':\n",
    "            li.append(this_paragh_words[i])\n",
    "    paragh_entity_name_words.append(li)\n",
    "print(paragh_entity_name_words)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "paragh_entity_word_set = []\n",
    "for i in range(len(paragh_entity_words)):\n",
    "    paragh_entity_word_set.append(set(paragh_entity_words[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "write() argument must be str, not bytes",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-d77b88e1544b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'entity_set.pickle'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparagh_entity_word_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"success!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: write() argument must be str, not bytes"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('entity_set.pickle','w') as f:\n",
    "    pickle.dump(paragh_entity_word_set,f)\n",
    "    print(\"success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
